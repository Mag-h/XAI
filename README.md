# Responsible AI - XAI
This repository presents our research work on **interpretable machine learning models** — models that are *transparent by construction*, rather than relying solely on post-hoc explanations. 
The goal is to bridge the gap between **state-of-the-art performance** and **human interpretability**, ensuring that models are not only accurate but also trustworthy and understandable.

## 🔍 Motivation

Most modern deep learning models are powerful but often act as **black boxes**. 
Interpretability is critical for:
- **High-stakes applications** (physical security, healthcare, finance, safety)
- **Regulatory compliance** (e.g., GDPR, AI Act)
- **Human trust and adoption**

Our approach focuses on **models that embed interpretability into their architecture and design**, rather than applying explanations as an afterthought.

## 🏗️ Approach




## 📚 Publication

This research has been presented as a spotlight paper at the XAI4CV CVPR Workshop 2025: 

- [ExaM: Unsupervised Concept-Based Representation Learning to Better Explain Models in Vision Tasks](https://openaccess.thecvf.com/content/CVPR2025W/XAI4CV/html/Heritier_ExaM_Unsupervised_Concept-Based_Representation_Learning_to_Better_Explain_Models_in_CVPRW_2025_paper.html) – CVPR Workshop , 2025 


## 🚀 Repository Content


> ⚠️ Note: Due to intellectual property restrictions, industrial code implementations are not publicly shared here. Instead, this repository focuses on research concepts, references, and illustrative material.


## 🤝 Contributing

This repo is primarily intended as a **research showcase**. 
If you are working on similar topics and would like to discuss collaboration, feel free to connect.

---

## 📫 Contact

- [LinkedIn](https://www.linkedin.com/in/maguelonne-heritier-03ba9bb/)
- [Google Scholar](https://scholar.google.com/citations?hl=fr&user=OBIkP1AAAAAJ)

